# AI infrastructure that
developers love

Serverless AI inference, large-scale batch processing, sandboxed code
execution, and much more.

[Get Started](/signup) Contact Us

[](/blog/ramp-case-study) [](https://www.achira.ai/)

[](https://www.quora.com) [](https://scale.com)

[](/blog/substack-case-study)
[![AI2](data:image/svg+xml,%3csvg%20width='76'%20height='24'%20viewBox='0%200%2076%2024'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cg%20clip-
path='url\(%23clip0_64_152\)'%3e%3cpath%20d='M9.34029%209.7503H4.67015V5.30796H8.42905C8.93023%205.30796%209.34029%204.8979%209.34029%204.39672V0.637817H13.7826V5.30796C13.7826%207.76833%2011.7893%209.7503%209.34029%209.7503ZM4.67015%2010.2059H0V14.6483H3.7589C4.26009%2014.6483%204.67015%2015.0583%204.67015%2015.5595V19.3184H9.11248V14.6483C9.11248%2012.1879%207.11913%2010.2059%204.67015%2010.2059ZM19.6032%209.97811C19.102%209.97811%2018.692%209.56805%2018.692%209.06686V5.30796H14.2496V9.97811C14.2496%2012.4385%2016.243%2014.4204%2018.692%2014.4204H23.3621V9.97811H19.6032ZM9.56811%2019.3184V23.9886H14.0104V20.2297C14.0104%2019.7285%2014.4205%2019.3184%2014.9217%2019.3184H18.6806V14.8761H14.0104C11.5501%2014.8761%209.56811%2016.8694%209.56811%2019.3184Z'%20fill='%23DDFFDC'%20fill-
opacity='1'/%3e%3cpath%20d='M53.4788%203.79307H49.0479V0.626483H53.4788V3.79307ZM50.0275%206.4243H47.3621V9.75036H49.6858C50.073%209.75036%2050.392%2010.0693%2050.392%2010.4566V24H53.9003V10.206C53.9003%207.757%2052.317%206.4243%2050.0389%206.4243H50.0275ZM39.3089%200.626483L47.9886%2023.9886H44.298L41.9971%2017.7807H31.1419L28.841%2023.9886H25.1846L33.8642%200.626483H39.3089ZM40.915%2014.8761L36.5752%203.18937L32.2354%2014.8761H40.915ZM62.1812%2021.084L69.3118%2015.457C73.4579%2012.1879%2075.2804%209.90982%2075.2804%206.77741C75.2804%203.64499%2073.2073%200%2066.9539%200C60.7005%200%2056.9074%205.08021%2056.9074%209.75036H60.7005C60.7005%205.53583%2062.5685%202.98434%2066.9539%202.98434C71.3393%202.98434%2071.4874%204.86379%2071.4874%206.81158C71.4874%208.75937%2070.9634%209.76175%2068.9245%2011.4134L56.9757%2021.0726V23.9886H75.7019V21.0726H62.1699L62.1812%2021.084Z'%20fill='%23DDFFDC'%20fill-
opacity='1'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_64_152'%3e%3crect%20width='75.7019'%20height='24'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://allenai.org)

[](https://www.cartesia.ai)
[![Harvey](data:image/svg+xml,%3csvg%20width='100'%20height='30'%20viewBox='0%200%20100%2030'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cg%20clip-
path='url\(%23clip0_32_72\)'%3e%3cpath%20d='M16.8833%2021.1123L14.1079%2022.9956V23.1278H22.8965V22.9956L20.1211%2021.1123V2.01542L22.8965%200.132159V0H14.1079V0.132159L16.8833%202.01542V10.4075H6.01322V2.01542L8.78855%200.132159V0H0V0.132159L2.77533%202.01542V21.1123L0%2022.9956V23.1278H8.78855V22.9956L6.01322%2021.1123V11.8612H16.8833V21.1123ZM29.141%2023.4582C30.8921%2023.4582%2032.6432%2022.4339%2033.8656%2021.0132V23.1278H39.4163V22.9956L36.9053%2021.0463V11.5969C36.9053%208.39207%2034.6586%206.80617%2031.0573%206.80617C29.174%206.80617%2026.5969%207.43392%2024.9449%208.09471V12.1916H25.0771C27.0595%209.48238%2028.9427%208.12775%2030.826%208.12775C32.7093%208.12775%2033.8656%209.44934%2033.8656%2011.9273V13.5793L29.7687%2014.5374C26.0683%2015.3634%2024.4163%2016.8172%2024.4163%2019.0969C24.4163%2021.6079%2026.4648%2023.4582%2029.141%2023.4582ZM30.1982%2021.3436C28.7115%2021.3436%2027.6211%2020.2533%2027.6211%2018.7665C27.6211%2017.2797%2028.6123%2016.0903%2030.5617%2015.6608L33.8656%2014.9009V19.8568C32.478%2020.848%2031.2555%2021.3436%2030.1982%2021.3436ZM52.4339%2010.2423L59.0419%2023.5903H59.6366L66.1123%209.28414L68.0947%207.3348V7.20264H62.0485V7.30176L64.5595%209.11894L60.0991%2018.9317L55.2093%209.11894L57.6542%207.30176V7.20264H52.5C49.3612%207.20264%2047.2797%208.16079%2045.6278%2010.0441V6.97137L40.0771%208.25991V8.42511L42.5881%209.87885V21.0463L40.0771%2022.9956V23.1278H48.7996V22.9956L45.6278%2021.0463V11.5969C46.7841%2010.3084%2048.1718%209.71366%2049.6256%209.71366C50.5507%209.71366%2051.4097%209.84581%2052.4339%2010.2423ZM75.2643%2023.4582C78.304%2023.4582%2079.9559%2021.5749%2081.674%2019.7247L80.848%2018.9648C79.163%2020.5507%2077.8745%2021.2445%2076.1894%2021.2445C72.9515%2021.2445%2070.5396%2018.2709%2070.5396%2014.1079H81.5749V13.6123C81.5749%209.71366%2079.2621%206.80617%2075.4626%206.80617C71.2335%206.80617%2067.7643%2010.6057%2067.7643%2015.2974C67.7643%2020.022%2071.0683%2023.4582%2075.2643%2023.4582ZM78.2709%2012.6872H70.6388C71.0352%209.81278%2072.8524%208.16079%2074.9339%208.16079C77.0154%208.16079%2078.304%209.64758%2078.304%2012.0264C78.304%2012.3238%2078.304%2012.5551%2078.2709%2012.6872ZM83.9207%209.28414L90.7599%2022.9295L85.804%2030H89.5374L97.8634%209.28414L99.8458%207.3348V7.20264H93.7996V7.30176L96.3436%209.11894L92.2137%2019.13L87.1916%209.11894L89.6366%207.30176V7.20264H81.8062V7.3348L83.9207%209.28414Z'%20fill='%23DDFFDC'%20fill-
opacity='1'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_32_72'%3e%3crect%20width='99.8458'%20height='30'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://harvey.ai)

[![Lovable](/_app/immutable/assets/Lovable.CoQIjurW.svg)](https://lovable.dev)
[![You.com](/_app/immutable/assets/YouDotCom.V2KoO3lf.svg)](https://you.com)

[](/blog/suno-case-study)
[![Cognition](/_app/immutable/assets/Cognition.DodUjy-h.svg)](https://cognition.ai)

[](https://mistral.ai)
[![OpenPipe](/_app/immutable/assets/OpenPipeGreen.ClRCnlxx.svg)](https://openpipe.ai)

[](/blog/contextual-case-study)
[![Codegen](/_app/immutable/assets/Codegen.ibfW0-BL.svg)](https://codegen.com)

## Sub-second container starts

We built a Rust-based container stack from scratch so you can iterate as
quickly in the cloud as you can locally.

[View Docs](/docs/guide)

## Zero config files

Easily define hardware and container requirements next to your Python
functions.

[View Docs](/docs/guide/gpu)

## Scale to hundreds of GPUs in seconds

Never worry about hitting rate limits again. We autoscale containers for your
functions instantly.

[View Docs](/docs/guide/scale)

## Use Cases

### Generative AI Inference that scales with you

[View Examples](/docs/examples/trtllm_latency)

* * *

Fast cold boots

Load gigabytes of weights in seconds with our optimized container file system.

* * *

Bring your own code

Deploy anything from custom models to popular frameworks.

* * *

Seamless autoscaling

Handle bursty and unpredictable load by scaling to thousands of GPUs and back
down to zero.

Generate

* * *

Fast cold boots

* * *

Bring your own code

* * *

Seamless autoscaling

[View Examples](/docs/examples/trtllm_latency)

### Fine-tuning and training without managing infrastructure

[View Examples](/docs/examples/llm-finetuning)

* * *

Start training immediately

Provision Nvidia A100 and H100 GPUs in seconds. Your drivers and custom
packages are already there.

* * *

Never wait in line

Run as many experiments as you need to, in parallel. Stop paying for idle GPUs
when you‚Äôre done.

* * *

Cloud storage

Mount weights and data in distributed volumes, then access them wherever
they‚Äôre needed.

![Fine-tuning graphic](https://modal-cdn.com/tmp_d4wxk9j_76c56b72.webp)

* * *

Start training immediately

* * *

Never wait in line

* * *

Cloud storage

[View Examples](/docs/examples/llm-finetuning)

### Batch processing optimized for high-volume workloads

[View Examples](/docs/examples/s3_bucket_mount)

* * *

Supercomputing scale

Serverless, but for high-performance compute. Run things on massive amounts of
CPU and memory.

* * *

Serverless pricing

Pay only for resources consumed, by the second, as you spin up containers.

* * *

Powerful compute primitives

Simple fan-out parallelism that scales to thousands of containers, with a
single line of Python.

![Batch processing graphic](https://modal-cdn.com/tmpwql3zd3v_a00ed2e5.webp)

* * *

Supercomputing scale

* * *

Serverless pricing

* * *

Powerful compute primitives

[View Examples](/docs/examples/s3_bucket_mount)

Build anything with Modal

[Language Models ](/use-cases/language-models)[Image, Video, 3D ](/use-
cases/image-video-3d)[Audio Processing ](/use-cases/audio)[Fine-Tuning ](/use-
cases/fine-tuning)[Batch Processing ](/use-cases/job-queues)[Sandboxed Code
](/use-cases/sandboxes)[Computational Bio ](/use-cases/comp-bio)

[Language Models ](/use-cases/language-models)[Image, Video, 3D ](/use-
cases/image-video-3d)[Audio Processing ](/use-cases/audio)[Fine-Tuning ](/use-
cases/fine-tuning)[Batch Processing ](/use-cases/job-queues)[Sandboxed Code
](/use-cases/sandboxes)[Computational Bio ](/use-cases/comp-bio)

## Features

Flexible Environments

Bring your own image or build one in Python, scale resources as needed, and
leverage state-of-the-art GPUs like H100s & A100s for high-performance
computing.

Seamless Integrations

Export function logs to Datadog or any OpenTelemetry-compatible provider, and
easily mount cloud storage from major providers (S3, R2 etc.).

Data Storage

Manage data effortlessly with storage solutions (network volumes, key-value
stores and queues). Provision storage types and interact with them using
familiar Python syntax.

Job Scheduling

Take control of your workloads with powerful scheduling. Set up cron jobs,
retries, and timeouts, or use batching to optimize resource usage.

Web Endpoints

Deploy and manage web services with ease. Create custom domains, set up
streaming and websockets, and serve functions as secure HTTPS endpoints.

Built-In Debugging

Troubleshoot efficiently with built-in debugging tools. Use the modal shell
for interactive debugging and set breakpoints to pinpoint issues quickly.

* * *

Flexible Environments

* * *

Seamless Integrations

* * *

Data Storage

* * *

Job Scheduling

* * *

Web Endpoints

* * *

Built-In Debugging

Only pay when your
code is running

Scale up to hundreds of nodes and down to zero within seconds. Pay for actual
compute, by the CPU cycle. With $30 of compute on us, every month.

### Compute costs

Per hour

Per second

Per hour

Per second

* * *

GPU Tasks

Nvidia B200

$0.001736 / sec

Nvidia H200

$0.001261 / sec

Nvidia H100

$0.001097 / sec

Nvidia A100, 80 GB

$0.000694 / sec

Nvidia A100, 40 GB

$0.000583 / sec

Nvidia L40S

$0.000542 / sec

Nvidia A10

$0.000306 / sec

Nvidia L4

$0.000222 / sec

Nvidia T4

$0.000164 / sec

* * *

CPU

Physical core
(2 vCPU equivalent)

$0.0000131 / core / sec

*minimum of 0.125 cores per container

* * *

Memory

$0.00000222 / GiB / sec

For teams
of all scales

Starter

For small teams and independent developers looking to level up.

Team

For startups and larger organizations looking to scale quickly.

Enterprise

For organizations prioritizing security, support, and reliability.

[View Pricing](/pricing)

## Security and governance

* * *

Built on top of gVisor

* * *

SOC 2 and HIPAA

* * *

Region support

* * *

SSO sign in for enterprise

[Learn More](/docs/guide/security)

* * *

Built on top of gVisor

* * *

SOC 2 and HIPAA

* * *

Region support

* * *

SSO sign in for enterprise

[Learn More](/docs/guide/security)

## Built with Modal

[View all](/docs/examples)

### [Deploy an OpenAI-compatible LLM service Run large language models with a
drop-in replacement for the OpenAI API ](/docs/examples/vllm_inference)

### [Custom pet art from Flux with Hugging Face and Gradio Fine-tune an image
generation model on pictures of your pet ](/docs/examples/dreambooth_app)

### [Run llama.cpp Run DeepSeek-R1 and Phi-4 on llama.cpp
](/docs/examples/llama_cpp)

### [Sandbox a LangGraph agent's code Run an LLM coding agent that runs its
own language models ](/docs/examples/agent)

### [Transcribe speech in batches with Whisper Turn audio bytes into text at
scale ](/docs/examples/batched_whisper)

### [Voice chat with LLMs Build an interactive voice chat app
](/docs/examples/llm-voice-chat)

### [Edit images with Flux Kontext Transform images with SotA diffusion models
](/docs/examples/image_to_image)

### [Fold proteins with Boltz-2 Predict molecular structures and binding
affinities from sequences with SotA open source models
](/docs/examples/boltz_predict)

### [Serverless WebRTC Stream YOLO detections on webcam footage in real time
](/docs/examples/webrtc_yolo)

### [Serve diffusion models Serve Flux on Modal with optimizations for
blazingly fast inference ](/docs/examples/flux)

### [Serverless TensorRT-LLM (LLaMA 3 8B) Run interactive language model
applications ](/docs/examples/trtllm_latency)

### [Transcribe speech with Kyutai STT Stream transcripts at the speed of
speech ](/docs/examples/streaming_kyutai_stt)

### [Star in custom music videos Fine-tune a Wan2.1 video model on your face
and run it in parallel ](/docs/examples/music-video-gen)

### [Create music Turn prompts into music with MusicGen
](/docs/examples/musicgen)

### [RAG Chat with PDFs Use ColBERT-style, multimodal embeddings with a
Vision-Language Model to answer questions about documents
](/docs/examples/chat_with_pdf_vision)

### [Bring images to life Prompt a generative video model to animate an image
](/docs/examples/image_to_video)

### [Fast podcast transcriptions Build an end-to-end podcast transcription app
that leverages dozens of containers for super-fast processing
](/docs/examples/whisper-transcriber)

### [Build a protein folding dashboard Serve a web UI for a protein model with
ESM3, Molstar, and Gradio ](/docs/examples/esm3)

### [Deploy a Hacker News Slackbot Periodically post new Hacker News posts to
Slack ](/docs/examples/hackernews_alerts)

### [Fold proteins with Chai-1 Predict molecular structures from sequences
with SotA open source models ](/docs/examples/chai1)

### [Retrieval-Augmented Generation (RAG) for Q&A Build a question-answering
web endpoint that can cite its sources ](/docs/examples/potus_speech_qanda)

### [Document OCR job queue Use Modal as an infinitely scalable job queue that
can service async tasks from a web app ](/docs/examples/doc_ocr_jobs)

### [Parallel processing of Parquet files on S3 Analyze data from the Taxi and
Limousine Commission of NYC in parallel ](/docs/examples/s3_bucket_mount)

> ‚ÄúModal Sandboxes enable us to execute generated code securely and flexibly.
> We expedited the development of our code interpreter feature integrated into
> Le Chat.‚Äù

Wendy Shang, AI Scientist

> ‚ÄúModal makes it easy to write code that runs on 100s of GPUs in parallel,
> transcribing podcasts in a fraction of the time.‚Äù

Mike Cohen, Head of Data

> ‚ÄúTasks that would have taken days to complete take minutes instead. We‚Äôve
> saved thousands of dollars deploying LLMs on Modal.‚Äù

Rahul Sengottuvelu, Head of Applied AI

> ‚ÄúThe beauty of Modal is that all you need to know is that you can scale your
> function calls in the cloud with a few lines of Python.‚Äù

Georg Kucsko, Co-founder and CTO

Join Modal's developer
community

[Modal Community Slack](/slack)

[![Twitter profile
@garrrikkotua](https://pbs.twimg.com/profile_images/1757781535267180544/TR1Coi0v.jpg)](https://twitter.com/garrrikkotua/status/1786042460143247506)
[Igor Kotua Engineer, The Linux
Foundation](https://twitter.com/garrrikkotua/status/1786042460143247506)

If you building AI stuff with Python and haven't tried
[@modal_labs](https://x.com/modal_labs) you are missing out big time

[![Twitter profile @danrothenberg](https://modal-
cdn.com/cdnbot/tmpndzqy7fc_1fe4563b.webp)](https://twitter.com/danrothenberg/status/1835055915516805301)
[Daniel Rothenberg Co-founder,
Brightband](https://twitter.com/danrothenberg/status/1835055915516805301)

[@modal_labs](https://x.com/modal_labs) continues to be magical... 10 minutes
of effort and the `joblib`-based parallelism I use to test on my local machine
can trivially scale out on the cloud. Makes life so easy!

[![Twitter profile
@erinselene](https://pbs.twimg.com/profile_images/1595233713536704513/j2d9PYiK.jpg)](https://twitter.com/erinselene/status/1601060264102678528)
[Erin Boyle ML Engineer,
Tesla](https://twitter.com/erinselene/status/1601060264102678528)

This tool is awesome. So empowering to have your infra needs met with just a
couple decorators. Good people, too!

[![Twitter profile
@jai_chopra](https://pbs.twimg.com/profile_images/1664774588772020225/TlVJPiQu.jpg)](https://twitter.com/jai_chopra/status/1661033887819268096)
[Jai Chopra Product,
LanceDB](https://twitter.com/jai_chopra/status/1661033887819268096)

Recently built an app on Lambda and just started to use
[@modal_labs](https://x.com/modal_labs), the difference is insane! Modal is
amazing, virtually no cold start time, onboarding experience is great üöÄ

[![Twitter profile
@dieegosf](https://pbs.twimg.com/profile_images/1623322737836949504/fYVRjQXS.jpg)](https://twitter.com/dieegosf/status/1811018060200874157)
[Diego Fernandes Co-founder & CTO,
RocketSeat](https://twitter.com/dieegosf/status/1811018060200874157)

Probably one of the best piece of software I'm using this year:
[modal.com](https://modal.com/)

[![Twitter profile
@AAAzzam](https://pbs.twimg.com/profile_images/1656519795976687619/abuB5K8p.jpg)](https://twitter.com/AAAzzam/status/1793118336525447302)
[Adam Azzam Product,
Prefect](https://twitter.com/AAAzzam/status/1793118336525447302)

feels weird at this point to use anything else than
[@modal_labs](https://x.com/modal_labs) for this ‚Äî absolutely the GOAT of
dynamic sandboxes

[![Twitter profile
@remilouf](https://pbs.twimg.com/profile_images/1570519314142318595/PJGWEkfu.jpg)](https://twitter.com/remilouf/status/1845742524997963800)
[R√©mi üìé Co-founder & CEO,
.txt](https://twitter.com/remilouf/status/1845742524997963800)

Nothing beats [@modal_labs](https://x.com/modal_labs) when it comes to
deploying a quick POC

[![Twitter profile
@holdenmatt](https://pbs.twimg.com/profile_images/1675120765761654784/jJz2F_r9.jpg)](https://twitter.com/holdenmatt/status/1797695485479915795)
[Matt Holden
Founder](https://twitter.com/holdenmatt/status/1797695485479915795)

Late to the party, but finally playing with
[@modal_labs](https://x.com/modal_labs) to run some backend jobs. DX is sooo
nice (compared to Docker, Cloud Run, Lambda, etc). Just decorate a Python
function and deploy. And it's fast! Love it.

[![Twitter profile
@garrrikkotua](https://pbs.twimg.com/profile_images/1757781535267180544/TR1Coi0v.jpg)](https://twitter.com/garrrikkotua/status/1786042460143247506)
[Igor Kotua Engineer, The Linux
Foundation](https://twitter.com/garrrikkotua/status/1786042460143247506)

If you building AI stuff with Python and haven't tried
[@modal_labs](https://x.com/modal_labs) you are missing out big time

[![Twitter profile @danrothenberg](https://modal-
cdn.com/cdnbot/tmpndzqy7fc_1fe4563b.webp)](https://twitter.com/danrothenberg/status/1835055915516805301)
[Daniel Rothenberg Co-founder,
Brightband](https://twitter.com/danrothenberg/status/1835055915516805301)

[@modal_labs](https://x.com/modal_labs) continues to be magical... 10 minutes
of effort and the `joblib`-based parallelism I use to test on my local machine
can trivially scale out on the cloud. Makes life so easy!

[![Twitter profile
@erinselene](https://pbs.twimg.com/profile_images/1595233713536704513/j2d9PYiK.jpg)](https://twitter.com/erinselene/status/1601060264102678528)
[Erin Boyle ML Engineer,
Tesla](https://twitter.com/erinselene/status/1601060264102678528)

This tool is awesome. So empowering to have your infra needs met with just a
couple decorators. Good people, too!

[![Twitter profile
@jai_chopra](https://pbs.twimg.com/profile_images/1664774588772020225/TlVJPiQu.jpg)](https://twitter.com/jai_chopra/status/1661033887819268096)
[Jai Chopra Product,
LanceDB](https://twitter.com/jai_chopra/status/1661033887819268096)

Recently built an app on Lambda and just started to use
[@modal_labs](https://x.com/modal_labs), the difference is insane! Modal is
amazing, virtually no cold start time, onboarding experience is great üöÄ

[![Twitter profile
@dieegosf](https://pbs.twimg.com/profile_images/1623322737836949504/fYVRjQXS.jpg)](https://twitter.com/dieegosf/status/1811018060200874157)
[Diego Fernandes Co-founder & CTO,
RocketSeat](https://twitter.com/dieegosf/status/1811018060200874157)

Probably one of the best piece of software I'm using this year:
[modal.com](https://modal.com/)

[![Twitter profile
@AAAzzam](https://pbs.twimg.com/profile_images/1656519795976687619/abuB5K8p.jpg)](https://twitter.com/AAAzzam/status/1793118336525447302)
[Adam Azzam Product,
Prefect](https://twitter.com/AAAzzam/status/1793118336525447302)

feels weird at this point to use anything else than
[@modal_labs](https://x.com/modal_labs) for this ‚Äî absolutely the GOAT of
dynamic sandboxes

[![Twitter profile
@remilouf](https://pbs.twimg.com/profile_images/1570519314142318595/PJGWEkfu.jpg)](https://twitter.com/remilouf/status/1845742524997963800)
[R√©mi üìé Co-founder & CEO,
.txt](https://twitter.com/remilouf/status/1845742524997963800)

Nothing beats [@modal_labs](https://x.com/modal_labs) when it comes to
deploying a quick POC

[![Twitter profile
@holdenmatt](https://pbs.twimg.com/profile_images/1675120765761654784/jJz2F_r9.jpg)](https://twitter.com/holdenmatt/status/1797695485479915795)
[Matt Holden
Founder](https://twitter.com/holdenmatt/status/1797695485479915795)

Late to the party, but finally playing with
[@modal_labs](https://x.com/modal_labs) to run some backend jobs. DX is sooo
nice (compared to Docker, Cloud Run, Lambda, etc). Just decorate a Python
function and deploy. And it's fast! Love it.

[![Twitter profile
@calebfahlgren](https://pbs.twimg.com/profile_images/1716604301563289600/YycgFNAn.jpg)](https://twitter.com/calebfahlgren/status/1825733420976124199)
[Caleb ML Engineer, Hugging
Face](https://twitter.com/calebfahlgren/status/1825733420976124199)

Bullish on [@modal_labs](https://x.com/modal_labs) \- Great Docs + Examples \-
Healthy Free Plan (30$ free compute / month) \- Never have to worry about
infra / just Python

[![Twitter profile @mattzcarey](https://modal-
cdn.com/cdnbot/tmpbisrydal_4d61419e.webp)](https://twitter.com/mattzcarey/status/1806003178691006905)
[@mattzcarey.com on blsky AI Engineer,
StackOne](https://twitter.com/mattzcarey/status/1806003178691006905)

[@modal_labs](https://x.com/modal_labs) has got a bunch of stuff just worked
out this should be how you deploy python apps. wow

[![Twitter profile @_amankishore](https://modal-
cdn.com/aman_kishore.jpg)](https://twitter.com/_amankishore/status/1669845359634575360)
[Aman Kishore Research Engineer,
Harvey](https://twitter.com/_amankishore/status/1669845359634575360)

If you are still using AWS Lambda instead of
[@modal_labs](https://x.com/modal_labs) you're not moving fast enough

[![Twitter profile
@isidoremiller](https://pbs.twimg.com/profile_images/1679859339073404930/_PB_4LM0.jpg)](https://twitter.com/isidoremiller/status/1645953205480878080)
[Izzy Miller DevRel,
Hex](https://twitter.com/isidoremiller/status/1645953205480878080)

special shout out to [@modal_labs](https://x.com/modal_labs) and
[@_hex_tech](https://x.com/_hex_tech) for providing the crucial infrastructure
to run this! Modal is the coolest tool I‚Äôve tried in a really long time‚Äî
cannnot say enough good things.

[![Twitter profile
@marktenenholtz](https://pbs.twimg.com/profile_images/1468741945560289283/YZ3cOr_H.jpg)](https://twitter.com/marktenenholtz/status/1784348202545614937)
[Mark Tenenholtz Head of AI,
PredeloHQ](https://twitter.com/marktenenholtz/status/1784348202545614937)

I use [@modal_labs](https://x.com/modal_labs) because it brings me joy. There
isn't much more to it.

[![Twitter profile
@schrockn](https://pbs.twimg.com/profile_images/1452768656711032833/Vq8wBRJc.jpg)](https://twitter.com/schrockn/status/1787504282700255676)
[Nick Schrock Founder, Dagster
Labs](https://twitter.com/schrockn/status/1787504282700255676)

I have tried [@modal_labs](https://x.com/modal_labs) and am now officially
Modal-pilled. Great work [@bernhardsson](https://x.com/bernhardsson) and team.
Every hyperscalar should be trying this out and immediately pivoting their
compute teams' roadmaps to match this DX.

[![Twitter profile
@moinnadeem](https://pbs.twimg.com/profile_images/1252656439149105156/dU40XVKb.jpg)](https://twitter.com/moinnadeem/status/1814729047181832484)
[Moin Nadeem Co-founder,
Phonic](https://twitter.com/moinnadeem/status/1814729047181832484)

I've realized [@modal_labs](https://x.com/modal_labs) is actually a great fit
for ML training pipelines. If you're running model-based evals, why not just
call a serverless Modal function and have it evaluate your model on a separate
worker GPU? This makes evaluation during training really easy.

[![Twitter profile
@calebfahlgren](https://pbs.twimg.com/profile_images/1716604301563289600/YycgFNAn.jpg)](https://twitter.com/calebfahlgren/status/1825733420976124199)
[Caleb ML Engineer, Hugging
Face](https://twitter.com/calebfahlgren/status/1825733420976124199)

Bullish on [@modal_labs](https://x.com/modal_labs) \- Great Docs + Examples \-
Healthy Free Plan (30$ free compute / month) \- Never have to worry about
infra / just Python

[![Twitter profile @mattzcarey](https://modal-
cdn.com/cdnbot/tmpbisrydal_4d61419e.webp)](https://twitter.com/mattzcarey/status/1806003178691006905)
[@mattzcarey.com on blsky AI Engineer,
StackOne](https://twitter.com/mattzcarey/status/1806003178691006905)

[@modal_labs](https://x.com/modal_labs) has got a bunch of stuff just worked
out this should be how you deploy python apps. wow

[![Twitter profile @_amankishore](https://modal-
cdn.com/aman_kishore.jpg)](https://twitter.com/_amankishore/status/1669845359634575360)
[Aman Kishore Research Engineer,
Harvey](https://twitter.com/_amankishore/status/1669845359634575360)

If you are still using AWS Lambda instead of
[@modal_labs](https://x.com/modal_labs) you're not moving fast enough

[![Twitter profile
@isidoremiller](https://pbs.twimg.com/profile_images/1679859339073404930/_PB_4LM0.jpg)](https://twitter.com/isidoremiller/status/1645953205480878080)
[Izzy Miller DevRel,
Hex](https://twitter.com/isidoremiller/status/1645953205480878080)

special shout out to [@modal_labs](https://x.com/modal_labs) and
[@_hex_tech](https://x.com/_hex_tech) for providing the crucial infrastructure
to run this! Modal is the coolest tool I‚Äôve tried in a really long time‚Äî
cannnot say enough good things.

[![Twitter profile
@marktenenholtz](https://pbs.twimg.com/profile_images/1468741945560289283/YZ3cOr_H.jpg)](https://twitter.com/marktenenholtz/status/1784348202545614937)
[Mark Tenenholtz Head of AI,
PredeloHQ](https://twitter.com/marktenenholtz/status/1784348202545614937)

I use [@modal_labs](https://x.com/modal_labs) because it brings me joy. There
isn't much more to it.

[![Twitter profile
@schrockn](https://pbs.twimg.com/profile_images/1452768656711032833/Vq8wBRJc.jpg)](https://twitter.com/schrockn/status/1787504282700255676)
[Nick Schrock Founder, Dagster
Labs](https://twitter.com/schrockn/status/1787504282700255676)

I have tried [@modal_labs](https://x.com/modal_labs) and am now officially
Modal-pilled. Great work [@bernhardsson](https://x.com/bernhardsson) and team.
Every hyperscalar should be trying this out and immediately pivoting their
compute teams' roadmaps to match this DX.

[![Twitter profile
@moinnadeem](https://pbs.twimg.com/profile_images/1252656439149105156/dU40XVKb.jpg)](https://twitter.com/moinnadeem/status/1814729047181832484)
[Moin Nadeem Co-founder,
Phonic](https://twitter.com/moinnadeem/status/1814729047181832484)

I've realized [@modal_labs](https://x.com/modal_labs) is actually a great fit
for ML training pipelines. If you're running model-based evals, why not just
call a serverless Modal function and have it evaluate your model on a separate
worker GPU? This makes evaluation during training really easy.
